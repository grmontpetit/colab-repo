{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week3-Exercise2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sniggel/colab-repo/blob/master/Week3_Exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PafuNO5tokN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "e3612aa2-d5c8-41e5-800e-ca338a16d15e"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "## Load MNIST\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "## Give a new shape to the array without changing its data.\n",
        "## where: 60000 = nbInTrainingSet, 28 = width, 28 = height, 1 is dimension\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "\n",
        "## Give a new shape to the array without changing its data.\n",
        "## where: 10000 = nbInTestgSet, 28 = width, 28 = height, 1 is dimension\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  ## Add an initial 3*3 convolution layer\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  ## Add a 2*2 hidden pooling layer, reducing the image size by half\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  ## Add a 3*3 hidden convolution layer, relu returns 0 on negative values\n",
        "  # tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  ## Add a 2*2 hidden pooling layer, reducing the image size by half\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  ## Flattens the images to a 1D array (as we can see in the summary: (None, 1600) )\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "## Display the size and shape of the network, notice the image is reduced after\n",
        "## every MaxPooling layer\n",
        "model.summary()\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "\n",
        "## Prints the first 100 labels in the test set\n",
        "print(test_labels[:100])\n",
        "\n",
        "## Remove the final Convolution. What impact will this have on accuracy or\n",
        "## training time ?\n",
        "\n",
        "## Answer:\n",
        "## The training time is lower by a factor of almost 50% in some cases.\n",
        "## Loss is 2%~3% lower, which means 2%~3% less predictions were made.\n",
        "## Accuracy is almost the same."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               295040    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 296,970\n",
            "Trainable params: 296,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 54s 901us/sample - loss: 0.4345 - acc: 0.8452\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 54s 901us/sample - loss: 0.3128 - acc: 0.8859\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 53s 890us/sample - loss: 0.2782 - acc: 0.8973\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 53s 882us/sample - loss: 0.2496 - acc: 0.9076\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 53s 891us/sample - loss: 0.2285 - acc: 0.9144\n",
            "10000/10000 [==============================] - 3s 289us/sample - loss: 0.2748 - acc: 0.8992\n",
            "0.8992\n",
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0w0xQsF_vavO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}