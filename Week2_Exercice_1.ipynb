{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week2-Exercice_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sniggel/colab-repo/blob/master/Week2_Exercice_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tC6svjk7Wmw1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "97cacc7d-2a9f-4a2a-c462-6158a8bf659d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## debug: print tensor flow version\n",
        "# print(tf.__version__)\n",
        "\n",
        "## Define MNIST\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "## Load MNIST (60000 training images and labels, 10000 test images and labels)\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "## Print the lenght of the image at index 100 (all images have same proportions)\n",
        "# print('Image size: ' + str(len(training_images[100])) + 'x' + str(len(training_images[100][0])))\n",
        "## Print the training label of the image at index 100\n",
        "#print('Training label: ' + str(training_labels[100]))\n",
        "## Print 2d array of pixel values representing the image at index 100\n",
        "# print('Image data:')\n",
        "# print(training_images[100])\n",
        "\n",
        "## Display a training image\n",
        "# print('Normalized image:')\n",
        "# plt.imshow(training_images[100])\n",
        "\n",
        "## Normalize the training images RGP values\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "## Design the model\n",
        "## Sequential: Defines a SEQUENCE of layers in the neural network\n",
        "## Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
        "## Dense: Adds a layer of neurons. Each layer of neurons need an **activation function** to tell them what to do. There's lots of options, but just use these for now. \n",
        "## Relu: Effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
        "## Softmax: takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "## Build the model\n",
        "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "## Exercice 1\n",
        "classifications = model.predict(test_images)\n",
        "## Print probability of the test image beign in the corresponding label\n",
        "print(classifications[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5028 - acc: 0.8226\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.3753 - acc: 0.8653\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.3349 - acc: 0.8783\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3125 - acc: 0.8849\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.2930 - acc: 0.8912\n",
            "[2.5103877e-06 3.7555131e-07 1.0381846e-07 6.2323755e-07 3.9280945e-07\n",
            " 1.7882079e-02 2.3142442e-04 1.5556315e-01 7.4352574e-05 8.2624495e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}